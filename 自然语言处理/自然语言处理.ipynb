{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7117009,"sourceType":"datasetVersion","datasetId":4104489},{"sourceId":7181048,"sourceType":"datasetVersion","datasetId":4150599}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"（1）数据集构建","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\n\n# 加载词表\ndef load_vocab():\n    word_dict = {}\n    with open('/kaggle/input/afqmc-datset-zip1/afqmc-datset/vocab.txt') as f:\n        for idx, item in enumerate(f.readlines()):\n            word_dict[item.strip()] = idx\n\n    return word_dict\n\n# 加载数据\ndef load_dataset(data_path, is_test):\n    examples = []\n    with open(data_path) as f:\n        for line in f.readlines():\n            line = json.loads(line)\n            text_a = line[\"sentence1\"]\n            text_b = line[\"sentence2\"]\n            if is_test:\n                examples.append((text_a, text_b,))\n            else:\n                label = line[\"label\"]\n                examples.append((text_a, text_b, label))\n    return examples\n\ndef load_afqmc_data(path):\n    train_path = os.path.join(path, 'train.json')\n    dev_path = os.path.join(path, 'dev.json')\n    test_path = os.path.join(path, 'test.json')\n\n    train_data = load_dataset(train_path, False)\n    dev_data = load_dataset(dev_path, False)\n    test_data = load_dataset(test_path, True)\n    return train_data, dev_data, test_data\n\n# 字符转id\ndef words2id(example, word_dict):\n    cls_id = word_dict['[CLS]']\n    sep_id = word_dict['[SEP]']\n\n    text_a, text_b, label = example\n\n    # 将中文字符切分成单个字符\n    text_a = list(text_a)\n    text_b = list(text_b)\n\n    input_a = [word_dict[item] if item in word_dict else word_dict['[UNK]'] for item in text_a]\n    input_b = [word_dict[item] if item in word_dict else word_dict['[UNK]'] for item in text_b]\n    input_ids = [cls_id] + input_a + [sep_id] + input_b + [sep_id]\n    segment_id = [0] * (len(input_a) + 2) + [1] * (len(input_b) + 1)\n    return input_ids, segment_id, int(label)\n\n# Dataloader中的collate_fn函数\ndef collate_fn(batch_data, pad_val=0, max_seq_len=512):\n    input_ids, segment_ids, labels = [], [], []\n    max_len = 0\n    for example in batch_data:\n        input_id, segment_id, label = example\n        # 对数据序列进行截断\n        input_ids.append(input_id[:max_seq_len])\n        segment_ids.append(segment_id[:max_seq_len])\n        labels.append(label)\n        # 保存序列最大长度\n        max_len = max(max_len, len(input_id))\n    # 对数据序列进行填充至最大长度\n    for i in range(len(labels)):\n        input_ids[i] = input_ids[i] + [pad_val] * (max_len - len(input_ids[i]))\n        segment_ids[i] = segment_ids[i] + [pad_val] * (max_len - len(segment_ids[i]))\n    return (torch.as_tensor(input_ids), torch.as_tensor(segment_ids)), torch.as_tensor(labels)\n\n\n# 使用例子\nvocab = load_vocab()\ntrain_data, dev_data, test_data = load_afqmc_data('/kaggle/input/afqmc-datset-zip1/afqmc-datset/AFQMC/')\n\n# 将句子转换成id\nexample = train_data[0]\ninput_ids, segment_ids, label = words2id(example, vocab)\nprint(\"Input IDs:\", input_ids)\nprint(\"Segment IDs:\", segment_ids)\nprint(\"Label:\", label)\n\n# 构建mini-batch并进行对齐\nbatch_data = [words2id(example, vocab) for example in train_data[:2]]\nbatch_input, batch_label = collate_fn(batch_data)\nprint(\"Batch Input:\", batch_input)\nprint(\"Batch Label:\", batch_label)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T12:04:42.482873Z","iopub.execute_input":"2023-12-19T12:04:42.483238Z","iopub.status.idle":"2023-12-19T12:04:44.431951Z","shell.execute_reply.started":"2023-12-19T12:04:42.483207Z","shell.execute_reply":"2023-12-19T12:04:44.431028Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Input IDs: [1, 3802, 2975, 1051, 4947, 43, 852, 201, 699, 48, 22, 806, 33, 254, 399, 49, 89, 1114, 2, 1051, 4947, 9, 254, 399, 45, 195, 201, 89, 1114, 2]\nSegment IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nLabel: 0\nBatch Input: (tensor([[   1, 3802, 2975, 1051, 4947,   43,  852,  201,  699,   48,   22,  806,\n           33,  254,  399,   49,   89, 1114,    2, 1051, 4947,    9,  254,  399,\n           45,  195,  201,   89, 1114,    2],\n        [   1, 3802, 2975,  283, 4947,  178,   75, 1147,  450,    7,  218,    2,\n         3802, 2975,  283, 4947, 1147,  450,   40,   13,   10,  614,  356,    2,\n            0,    0,    0,    0,    0,    0]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0]]))\nBatch Label: tensor([0, 0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"（2）实现输入编码、分段编码和位置编码，并组装为嵌入层，打印该层的输入输出","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Embedding\n\nclass TransformerEmbedding(Layer):\n    def __init__(self, vocab_size, emb_size, max_position_embeddings=512):\n        super(TransformerEmbedding, self).__init__()\n\n        self.word_embedding = Embedding(vocab_size, emb_size)\n        self.position_embedding = Embedding(max_position_embeddings, emb_size)\n        self.segment_embedding = Embedding(2, emb_size)  # 0 for sentence A, 1 for sentence B\n\n    def call(self, inputs):\n        input_ids, segment_ids = inputs\n\n        # 输入编码\n        word_embeddings = self.word_embedding(input_ids)\n\n        # 分段编码\n        segment_embeddings = self.segment_embedding(segment_ids)\n\n        # 位置编码\n        position_ids = tf.range(tf.shape(input_ids)[1], dtype=tf.int32)\n        position_embeddings = self.position_embedding(position_ids)\n\n        # 将各个编码相加得到最终的嵌入表示\n        embeddings = word_embeddings + position_embeddings + segment_embeddings\n\n        return embeddings\n\n# 例子\nvocab_size = 10000  # 假设词汇表大小为10000\nemb_size = 300  # 假设词向量维度为300\n\n# 创建TransformerEmbedding实例\nembedding_layer = TransformerEmbedding(vocab_size, emb_size)\n\n# 构造一个mini-batch的输入数据\ninput_ids = tf.constant([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])  # 示例输入\nsegment_ids = tf.constant([[0, 0, 0, 1, 1], [0, 1, 1, 0, 1]])  # 示例分段标记\n\n# 使用嵌入层进行编码\nembeddings = embedding_layer([input_ids, segment_ids])\n\n# 打印输入和输出\nprint(\"Input IDs:\", input_ids.numpy())\nprint(\"Segment IDs:\", segment_ids.numpy())\nprint(\"Output Embeddings:\", embeddings.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T12:04:44.433544Z","iopub.execute_input":"2023-12-19T12:04:44.434108Z","iopub.status.idle":"2023-12-19T12:04:50.032418Z","shell.execute_reply.started":"2023-12-19T12:04:44.434080Z","shell.execute_reply":"2023-12-19T12:04:50.031432Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Input IDs: [[ 1  2  3  4  5]\n [ 6  7  8  9 10]]\nSegment IDs: [[0 0 0 1 1]\n [0 1 1 0 1]]\nOutput Embeddings: [[[-2.92048492e-02  2.50686556e-02 -1.63089242e-02 ...  6.18023798e-03\n   -4.84248511e-02 -6.03079572e-02]\n  [-8.10497999e-03  2.34684870e-02 -2.29900144e-02 ... -6.87457100e-02\n    2.10056081e-02  5.60190529e-05]\n  [ 7.74686784e-03 -3.25232334e-02 -6.60799146e-02 ... -1.33945048e-03\n   -4.31753881e-02 -4.71767075e-02]\n  [ 4.96294163e-02  2.52648033e-02  7.52567202e-02 ...  3.51153202e-02\n   -5.60104027e-02 -2.70643458e-03]\n  [ 3.93309407e-02 -1.60002857e-02 -6.06644750e-02 ...  3.81373316e-02\n    2.13111080e-02  2.35103257e-02]]\n\n [[ 8.45070928e-04  3.54717523e-02 -1.05034046e-01 ... -4.78850007e-02\n   -7.80276358e-02 -7.64850751e-02]\n  [ 2.38598958e-02  1.41861662e-02 -6.61173090e-02 ...  4.35464680e-02\n    6.63404986e-02  5.85090593e-02]\n  [-1.54073574e-02 -7.41142035e-02  2.34907418e-02 ...  6.66059405e-02\n    7.69123062e-03 -1.29668247e-02]\n  [ 1.35094039e-02  2.72904448e-02  4.33319509e-02 ... -2.59463787e-02\n   -4.48969975e-02 -3.09090633e-02]\n  [ 1.89174823e-02 -7.19348788e-02  3.15895900e-02 ...  5.86092994e-02\n   -1.99482422e-02 -3.11012194e-03]]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"（3）实现多头自注意力层和add&norm层","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Dense, Dropout\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, emb_size, num_heads, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n\n        self.emb_size = emb_size\n        self.num_heads = num_heads\n        self.head_dim = emb_size // num_heads\n\n        assert self.head_dim * num_heads == emb_size, \"Embedding size needs to be divisible by heads\"\n\n        self.query_linear = Dense(emb_size)\n        self.key_linear = Dense(emb_size)\n        self.value_linear = Dense(emb_size)\n\n        self.fc_out = Dense(emb_size)\n\n        self.dropout = Dropout(dropout)\n\n    def call(self, inputs):\n        query, key, value, mask = inputs\n        batch_size = tf.shape(query)[0]\n        seq_len = tf.shape(query)[1]\n\n        # 线性变换\n        Q = self.query_linear(query)\n        K = self.key_linear(key)\n        V = self.value_linear(value)\n\n        # 多头分组\n        Q = tf.transpose(tf.reshape(Q, (batch_size, seq_len, self.num_heads, self.head_dim)), perm=[0, 2, 1, 3])\n        K = tf.transpose(tf.reshape(K, (batch_size, seq_len, self.num_heads, self.head_dim)), perm=[0, 2, 1, 3])\n        V = tf.transpose(tf.reshape(V, (batch_size, seq_len, self.num_heads, self.head_dim)), perm=[0, 2, 1, 3])\n\n        # 形状重组\n        energy = tf.matmul(Q, tf.transpose(K, perm=[0, 1, 3, 2])) / tf.math.sqrt(tf.cast(self.head_dim, dtype=tf.float32))\n\n        # 掩码处理\n        if mask is not None:\n            mask = tf.expand_dims(tf.expand_dims(mask, axis=1), axis=2)  # Broadcasting the mask\n            energy = tf.where(mask == 0, tf.constant(float('-1e20'), dtype=tf.float32), energy)\n\n        # QKV注意力计算\n        attention = tf.nn.softmax(energy, axis=-1)\n        x = tf.matmul(self.dropout(attention), V)\n\n        # 重组恢复\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        x = tf.reshape(x, (batch_size, seq_len, self.emb_size))\n\n        # 多头融合\n        x = self.fc_out(x)\n\n        return x\n\nclass AddNorm(Layer):\n    def __init__(self, emb_size, mlp_units, dropout=0.1):\n        super(AddNorm, self).__init__()\n\n        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout_layer = Dropout(dropout)\n\n        # 添加线性层，将 MLP 输出维度转换为 emb_size\n        self.mlp_linear = Dense(emb_size)\n\n    def call(self, inputs, training=None):\n        x, sublayer, mask = inputs\n\n        # 残差连接\n        residual = x\n\n        # 确保 sublayer 是一个可以调用的层对象\n        sublayer_output = sublayer([x, x, x, mask]) if callable(sublayer) else sublayer\n\n        # 更新这行代码，确保在训练时使用 dropout\n        mlp_output = self.mlp_linear(sublayer_output)\n        x = residual + self.dropout_layer(mlp_output, training=training)\n\n        # 层规范化\n        x = self.norm(x)\n        return x\n\n\n\n\n# 示例\nemb_size = 300\nnum_heads = 6\ndropout = 0.1\n\n# 创建MultiHeadAttention实例\nattention_layer = MultiHeadAttention(emb_size, num_heads, dropout)\n\n# 创建示例输入\nquery = tf.random.normal((2, 10, emb_size))\nkey = tf.random.normal((2, 10, emb_size))\nvalue = tf.random.normal((2, 10, emb_size))\nmask = tf.ones((2, 10))\n\n# 多头自注意力层\noutput = attention_layer([query, key, value, mask])\n\n# 打印输入和输出\nprint(\"Query shape:\", query.shape)\nprint(\"Output shape:\", output.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T12:04:50.033670Z","iopub.execute_input":"2023-12-19T12:04:50.033988Z","iopub.status.idle":"2023-12-19T12:04:52.020917Z","shell.execute_reply.started":"2023-12-19T12:04:50.033962Z","shell.execute_reply":"2023-12-19T12:04:52.019994Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Query shape: (2, 10, 300)\nOutput shape: (2, 10, 300)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"（4）搭建一个transformer编码器，利用嵌入层、transformer编码器和合适的分类器构建完成语义匹配模型","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n\n# 构建Transformer模型\ndef build_transformer_model(vocab_size, emb_size, num_heads, num_transformer_blocks, mlp_units, dropout, max_position_embeddings=512):\n    # 输入层\n    input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n    segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n\n    # 嵌入层\n    embeddings = TransformerEmbedding(vocab_size, emb_size, max_position_embeddings)([input_ids, segment_ids])\n\n    # Transformer 编码器\n    for _ in range(num_transformer_blocks):\n        attention_output = MultiHeadAttention(emb_size, num_heads, dropout)([embeddings, embeddings, embeddings, None])\n        # 加和归一化层\n        attention_output = AddNorm(emb_size, dropout)([embeddings, attention_output, None])\n        # Feed Forward\n        mlp_output = Dense(mlp_units, activation=\"relu\")(attention_output)\n        # 加和归一化层\n        embeddings = AddNorm(emb_size, dropout)([attention_output, mlp_output, None])\n\n    # 池化层\n    pooled = GlobalAveragePooling1D()(embeddings)\n\n    # 分类器\n    outputs = Dense(1, activation=\"sigmoid\")(pooled)\n\n    # 构建模型\n    model = tf.keras.Model(inputs=[input_ids, segment_ids], outputs=outputs)\n\n    return model\n\n# 构建语义匹配模型\nsemantic_matching_model = build_transformer_model(vocab_size, emb_size, num_heads=6, num_transformer_blocks=4, mlp_units=512, dropout=0.1)\n\n# 打印模型组成\nsemantic_matching_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T12:04:52.023082Z","iopub.execute_input":"2023-12-19T12:04:52.023392Z","iopub.status.idle":"2023-12-19T12:04:53.537244Z","shell.execute_reply.started":"2023-12-19T12:04:52.023353Z","shell.execute_reply":"2023-12-19T12:04:53.536315Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, None)]               0         []                            \n                                                                                                  \n input_2 (InputLayer)        [(None, None)]               0         []                            \n                                                                                                  \n transformer_embedding_1 (T  (None, None, 300)            3154200   ['input_1[0][0]',             \n ransformerEmbedding)                                                'input_2[0][0]']             \n                                                                                                  \n multi_head_attention_1 (Mu  (None, None, 300)            361200    ['transformer_embedding_1[0][0\n ltiHeadAttention)                                                  ]',                           \n                                                                     'transformer_embedding_1[0][0\n                                                                    ]',                           \n                                                                     'transformer_embedding_1[0][0\n                                                                    ]']                           \n                                                                                                  \n add_norm (AddNorm)          (None, None, 300)            90900     ['transformer_embedding_1[0][0\n                                                                    ]',                           \n                                                                     'multi_head_attention_1[0][0]\n                                                                    ']                            \n                                                                                                  \n dense_9 (Dense)             (None, None, 512)            154112    ['add_norm[0][0]']            \n                                                                                                  \n add_norm_1 (AddNorm)        (None, None, 300)            154500    ['add_norm[0][0]',            \n                                                                     'dense_9[0][0]']             \n                                                                                                  \n multi_head_attention_2 (Mu  (None, None, 300)            361200    ['add_norm_1[0][0]',          \n ltiHeadAttention)                                                   'add_norm_1[0][0]',          \n                                                                     'add_norm_1[0][0]']          \n                                                                                                  \n add_norm_2 (AddNorm)        (None, None, 300)            90900     ['add_norm_1[0][0]',          \n                                                                     'multi_head_attention_2[0][0]\n                                                                    ']                            \n                                                                                                  \n dense_16 (Dense)            (None, None, 512)            154112    ['add_norm_2[0][0]']          \n                                                                                                  \n add_norm_3 (AddNorm)        (None, None, 300)            154500    ['add_norm_2[0][0]',          \n                                                                     'dense_16[0][0]']            \n                                                                                                  \n multi_head_attention_3 (Mu  (None, None, 300)            361200    ['add_norm_3[0][0]',          \n ltiHeadAttention)                                                   'add_norm_3[0][0]',          \n                                                                     'add_norm_3[0][0]']          \n                                                                                                  \n add_norm_4 (AddNorm)        (None, None, 300)            90900     ['add_norm_3[0][0]',          \n                                                                     'multi_head_attention_3[0][0]\n                                                                    ']                            \n                                                                                                  \n dense_23 (Dense)            (None, None, 512)            154112    ['add_norm_4[0][0]']          \n                                                                                                  \n add_norm_5 (AddNorm)        (None, None, 300)            154500    ['add_norm_4[0][0]',          \n                                                                     'dense_23[0][0]']            \n                                                                                                  \n multi_head_attention_4 (Mu  (None, None, 300)            361200    ['add_norm_5[0][0]',          \n ltiHeadAttention)                                                   'add_norm_5[0][0]',          \n                                                                     'add_norm_5[0][0]']          \n                                                                                                  \n add_norm_6 (AddNorm)        (None, None, 300)            90900     ['add_norm_5[0][0]',          \n                                                                     'multi_head_attention_4[0][0]\n                                                                    ']                            \n                                                                                                  \n dense_30 (Dense)            (None, None, 512)            154112    ['add_norm_6[0][0]']          \n                                                                                                  \n add_norm_7 (AddNorm)        (None, None, 300)            154500    ['add_norm_6[0][0]',          \n                                                                     'dense_30[0][0]']            \n                                                                                                  \n global_average_pooling1d (  (None, 300)                  0         ['add_norm_7[0][0]']          \n GlobalAveragePooling1D)                                                                          \n                                                                                                  \n dense_32 (Dense)            (None, 1)                    301       ['global_average_pooling1d[0][\n                                                                    0]']                          \n                                                                                                  \n==================================================================================================\nTotal params: 6197349 (23.64 MB)\nTrainable params: 6197349 (23.64 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"（5）训练模型，在验证集上计算准确率，并保存在验证集上准确率最高的模型，使用tensorboard等可视化插件，展示训练过程中的精度变化和损失变化","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\nimport numpy as np\n\n# 定义TensorBoard回调\ntensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n\n# 定义模型\nsemantic_matching_model = build_transformer_model(vocab_size, emb_size, num_heads=6, num_transformer_blocks=4, mlp_units=512, dropout=0.1)\n\n# 编译模型\nsemantic_matching_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# 加载和处理训练集数据\ntrain_data, dev_data, test_data = load_afqmc_data('/kaggle/input/afqmc-datset-zip1/afqmc-datset/AFQMC/')\n\n# 将训练集数据处理成模型输入\ntrain_inputs, train_labels = collate_fn([words2id(example, vocab) for example in train_data])\n\n# 转换为numpy数组\ntrain_inputs = (np.array(train_inputs[0]), np.array(train_inputs[1]))\ntrain_labels = np.array(train_labels)\n\n# 准备验证数据\ndev_inputs, dev_labels = collate_fn([words2id(example, vocab) for example in dev_data])\n\n# 转换为numpy数组\ndev_inputs = (np.array(dev_inputs[0]), np.array(dev_inputs[1]))\ndev_labels = np.array(dev_labels)\n\n# 定义模型保存路径\nmodel_checkpoint = ModelCheckpoint(\n    './logs/best_model.h5',  # 模型保存路径\n    save_best_only=True,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    verbose=1\n)\n\n# 定义早停策略\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=3,\n    mode='max',\n    verbose=1\n)\n\n# 训练模型\nhistory = semantic_matching_model.fit(\n    train_inputs,\n    train_labels,\n    epochs=10,\n    batch_size=32,\n    validation_data=(dev_inputs, dev_labels),\n    callbacks=[tensorboard_callback, model_checkpoint, early_stopping]\n)\n\n# 评估模型\neval_loss, eval_accuracy = semantic_matching_model.evaluate(dev_inputs, dev_labels)\nprint(f\"Evaluation Loss: {eval_loss}, Evaluation Accuracy: {eval_accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T12:16:07.406241Z","iopub.execute_input":"2023-12-19T12:16:07.407176Z","iopub.status.idle":"2023-12-19T12:20:57.819655Z","shell.execute_reply.started":"2023-12-19T12:16:07.407138Z","shell.execute_reply":"2023-12-19T12:20:57.818675Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1073/1073 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.6880\nEpoch 1: val_accuracy improved from -inf to 0.68999, saving model to ./logs/best_model.h5\n1073/1073 [==============================] - 89s 68ms/step - loss: 0.6405 - accuracy: 0.6880 - val_loss: 0.6195 - val_accuracy: 0.6900\nEpoch 2/10\n1073/1073 [==============================] - ETA: 0s - loss: 0.6225 - accuracy: 0.6921\nEpoch 2: val_accuracy did not improve from 0.68999\n1073/1073 [==============================] - 64s 60ms/step - loss: 0.6225 - accuracy: 0.6921 - val_loss: 0.6237 - val_accuracy: 0.6900\nEpoch 3/10\n1073/1073 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.6921\nEpoch 3: val_accuracy did not improve from 0.68999\n1073/1073 [==============================] - 63s 59ms/step - loss: 0.6198 - accuracy: 0.6921 - val_loss: 0.6191 - val_accuracy: 0.6900\nEpoch 4/10\n1073/1073 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.6921\nEpoch 4: val_accuracy did not improve from 0.68999\n1073/1073 [==============================] - 63s 59ms/step - loss: 0.6194 - accuracy: 0.6921 - val_loss: 0.6241 - val_accuracy: 0.6900\nEpoch 4: early stopping\n135/135 [==============================] - 3s 20ms/step - loss: 0.6241 - accuracy: 0.6900\nEvaluation Loss: 0.6240836381912231, Evaluation Accuracy: 0.689990758895874\n","output_type":"stream"}]},{"cell_type":"markdown","source":"（6）加载保存的模型，在测试集上随机选取50条数据进行语义匹配测试，展示模型的预测结果","metadata":{}},{"cell_type":"code","source":"import random\nimport tensorflow as tf\nimport numpy as np\n\n# 随机选取50条样本\nrandom.seed(42)  # 设置随机种子以保持可重复性\nselected_samples = random.sample(test_data, 50)\n\n# 获取特殊标记的id\ncls_id = vocab['[CLS]']\nsep_id = vocab['[SEP]']\n\n# 加载模型\nmodel_path = '/kaggle/working/logs/best_model.h5'  # 模型保存路径\nloaded_model = build_transformer_model(vocab_size, emb_size, num_heads=6, num_transformer_blocks=4, mlp_units=512, dropout=0.1)\nloaded_model.load_weights(model_path)\n\n# 对每条样本进行测试\nfor example in selected_samples:\n    text_a = example[0]\n    text_b = example[1]\n\n    # 转换成id的形式\n    input_ids_a = [vocab.get(item, vocab['[UNK]']) for item in list(text_a)]\n    input_ids_b = [vocab.get(item, vocab['[UNK]']) for item in list(text_b)]\n\n    input_ids = [cls_id] + input_ids_a + [sep_id] + input_ids_b + [sep_id]\n    segment_ids = [0] * (len(input_ids_a) + 2) + [1] * (len(input_ids_b) + 1)\n\n    # 转换成Tensor张量\n    input_ids = tf.convert_to_tensor([input_ids])\n    segment_ids = tf.convert_to_tensor([segment_ids])\n    inputs = [input_ids, segment_ids]\n\n    # 模型预测\n    logits = loaded_model.predict(inputs)\n\n    # 取概率值最大的索引\n    label_id = np.argmax(logits, axis=1)[0]\n\n    # 打印预测结果\n    print(f\"文本A: {text_a}\")\n    print(f\"文本B: {text_b}\")\n    print(f\"预测的label标签: {label_id}\")\n    print(\"=\" * 50)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T12:33:58.198754Z","iopub.execute_input":"2023-12-19T12:33:58.199148Z","iopub.status.idle":"2023-12-19T12:34:04.379467Z","shell.execute_reply.started":"2023-12-19T12:33:58.199116Z","shell.execute_reply":"2023-12-19T12:34:04.378661Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 1s/step\n文本A: 花呗叫绑定银行卡是怎么回事\n文本B: 蚂蚁花呗老是提示绑定银行卡是什么原因\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 1s 946ms/step\n文本A: 花呗如何关\n文本B: 如何确定我是否关闭花呗成功\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 借呗额度怎么越来越少\n文本B: 我借呗额度怎么突然降低了？什么情况\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 有蚂蚁借呗都开通不了\n文本B: 我蚂蚁借呗为何不通过\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 为什么花呗我有记录\n文本B: 我的花呗记录有疑问\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 蚂蚁借呗怎么认证不了\n文本B: 我已实名认证但我还不能蚂蚁借呗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 申请的商家收款码。怎么开通花呗信用卡\n文本B: 我的店铺怎么开通花呗付款\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 蚂蚁借呗分期还款可以更改吗\n文本B: 可以查看朋友的借呗还款日吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 借呗借***个月，利息是多少\n文本B: 蚂蚁借呗的一个月的利率\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 经开通了花呗功能\n文本B: 我能申请开通蚂蚁花呗吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗***号还款我忘了\n文本B: 我双十一用花呗付款了，我想还款可以么\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 为什么花呗不能在实体店里支付\n文本B: 为什么我这上面没有花呗支付\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 我花呗逾期***多天会怎么\n文本B: 花呗如果逾期***天未还款\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 蚂蚁借呗还可以分期嘛\n文本B: 我用的是蚂蚁借呗，能不能分期\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 我提前两天还了借呗怎么还是显示我逾期\n文本B: 蚂蚁借呗超一天还款算是逾期吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 怎么申请开通借呗\n文本B: 我的借呗开通不了\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 我的支付宝花呗为什么淘宝用不了\n文本B: 为什么我的淘宝和拼多多不能用花呗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 支付宝花呗最迟还款几号\n文本B: 我的花呗是几号还款日\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 如何查看花呗详单\n文本B: 花呗怎么查看分期降单\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 26ms/step\n文本A: 借呗还款日怎么规定的\n文本B: 怎么样更改借呗还款日\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 我在借呗借的钱已经还了，为什么借呗不显示额度了\n文本B: 怎么借呗不显示借钱额度\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 怎么借呗在手机上不见了\n文本B: 为什么版本更新了借呗不见了\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 蚂蚁借呗还款手续费是什么\n文本B: 为什么借呗的手续费贵\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 滴滴可以用花呗\n文本B: 之前都是可以花呗付滴滴车费\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 借呗分期的、怎么还款\n文本B: 蚂蚁借呗如果不还款，会怎么样\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗是否支持水电煤缴费\n文本B: 水电煤可以用花呗吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗！还款日可以延迟吗\n文本B: 蚂蚁花呗借这个钱可不可以延期还款\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗绑定银行卡 绑定过了 为什么还显示绑定\n文本B: 我的花呗为什么老提示绑定银行卡\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗账单分期了，我能取消吗\n文本B: 我刚刚花呗分期了能不能取消分期呀\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗临时额度可以分期还吗？应该怎么申请\n文本B: 双十二花呗可以给临时额度么\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 我借呗能怎么提额度\n文本B: 能不能帮我借呗提一下额度\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 我这个月的蚂蚁借呗何时还款\n文本B: 蚂蚁借呗还款最高期限是多久\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 我为什么无法使用花呗分期\n文本B: 为什么我的花呗不可以二次分期\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 蚂蚁花呗一天自动扣款几次\n文本B: 花呗几号自动扣款\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 26ms/step\n文本A: 到店花呗消费返现资格\n文本B: 花呗支付返现\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 借呗有逾期，额度太多可以分期还款吗\n文本B: 蚂蚁借呗可以再分期还款吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 我得花呗和借呗都使用不起\n文本B: 为什么我的借呗和花呗都没有了\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 花呗分期付款后当月还需要还款么\n文本B: 花呗分期还款好吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 美团为什么不能使用花呗付款\n文本B: 为什么在美团外卖上用不了花呗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗怎么设支付密码\n文本B: 花呗怎么設密码\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 23ms/step\n文本A: 蚂蚁花呗冻结了还能开通吗\n文本B: 花呗冻结什么时候可以开通\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 我的花呗逾期了，但现在全款还完了，花呗还可以继续使用吗\n文本B: 我花呗有逾期过，还款了还能用吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 花呗还款扣了两次钱\n文本B: 我是绑定的银行卡，每次银行卡里的钱也扣了，怎么花呗还让我还款？那我不是出了两次钱吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗开通了，但暂时没有用，不会有费用吧\n文本B: 花呗申请成功，不使用会不会产生费用\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗账单分期免息券查不到\n文本B: 现在还有花呗分期免息的券吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 花呗从复扣款\n文本B: 中午有扣款花呗吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 花呗一次最高消费能用多少\n文本B: 花呗单笔支付额对最高多少\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 25ms/step\n文本A: 我昨天在淘宝用花呗付款，可是我进入花呗显示我没开通花呗\n文本B: 我在淘宝上使用了蚂蚁花呗 可是支付宝上显示不能使用花呗怎么回事\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 24ms/step\n文本A: 还借呗的钱可以从余额宝里直接扣除吗\n文本B: 余额宝可以还蚂蚁借呗吗？可以自动还款吗\n预测的label标签: 0\n==================================================\n1/1 [==============================] - 0s 26ms/step\n文本A: 借呗按期还款会有逾期吗\n文本B: 我之前借呗忘还了，逾期了几天有事吗\n预测的label标签: 0\n==================================================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"（7）输入一条样本提取多头注意力权重，对注意力机制的计算结果进行可视化展示并分析","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Choose a sample index\nsample_index = 0\ntext_a, text_b, true_label = test_data[sample_index]\n\n# Convert true_label to integer\ntrue_label = int(true_label)\n\n# Prepare data\ninput_ids, segment_ids, labels = collate_fn([(text_a, text_b, true_label)])\ninput_ids, segment_ids, labels = input_ids.to(device), segment_ids.to(device), labels.to(device)\n\n# Get attention weights\nwith torch.no_grad():\n    _, attention_weights = model(input_ids, segment_ids, mask=None)\n\n# Print attention weights\nprint(\"Attention Weights:\")\nfor head in range(attention_weights.size(1)):\n    print(f\"Head {head + 1}: {attention_weights[0][head].mean().item()}\")\n\n# Visualize attention weights\nsns.set(font_scale=1.2)\nplt.figure(figsize=(12, 8))\n\n# Combine text_a and text_b for visualization\ncombined_text = [\"[CLS]\"] + list(jieba.cut(\" \".join(map(str, text_a)))) + [\"[SEP]\"] + list(jieba.cut(\" \".join(map(str, text_b)))) + [\"[SEP]\"]\n\nax = sns.heatmap(\n    attention_weights[0][0].cpu().numpy(),\n    cmap=\"YlGnBu\",\n    xticklabels=combined_text,\n    yticklabels=combined_text,\n    annot=True,\n    fmt=\".2f\"\n)\nax.set_title(\"Attention Weight Visualization\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T12:12:38.862555Z","iopub.status.idle":"2023-12-19T12:12:38.862882Z","shell.execute_reply.started":"2023-12-19T12:12:38.862723Z","shell.execute_reply":"2023-12-19T12:12:38.862738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"10.层规范化的位置有两种 prenorm 和 postnorm，查询资料了解二者区别并说明自己的模型中层规范化操作的位置是 prenorm 还是 postnorm ，然后尝试另一种层规范化操作，对比二者在具体训练中的区别并分析原因。","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, embed_size, max_position_embeddings, num_heads, num_layers, mlp_units, dropout_rate):\n        super(TransformerEncoder, self).__init__()\n\n        self.embedding_layer = TransformerEmbedding(vocab_size, embed_size, max_position_embeddings)\n        self.transformer_blocks = [TransformerBlock(embed_size, num_heads, dropout_rate) for _ in range(num_layers)]\n        self.mlp_units = mlp_units\n        self.dropout_rate = dropout_rate\n\n    def call(self, inputs, training=None):\n        input_ids, segment_ids = inputs\n\n        # 嵌入层\n        embeddings = self.embedding_layer([input_ids, segment_ids])\n\n        # Transformer 编码器\n        for block in self.transformer_blocks:\n            attention_output = block([embeddings, embeddings, embeddings, None])\n            # 加和归一化层\n            embeddings = AddNorm(embeddings, attention_output, None, self.dropout_rate)\n\n        return embeddings\n\n# 修改 AddNorm 层的 call 方法\nclass AddNorm(tf.keras.layers.Layer):\n    def __init__(self, emb_size, dropout_rate):\n        super(AddNorm, self).__init__()\n\n        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout_layer = Dropout(dropout_rate)\n\n    def call(self, inputs, training=None):\n        x, sublayer, mask = inputs\n\n        # 残差连接\n        residual = x\n\n        # 更新这行代码，确保在训练时使用 dropout\n        mlp_output = self.mlp_linear(sublayer(x))\n        x = residual + self.dropout_layer(mlp_output, training=training)\n\n        # 层规范化\n        x = self.norm(x)\n        return x\n\n# 创建语义匹配模型\nclass SemanticMatchingModel(tf.keras.Model):\n    def __init__(self, vocab_size, embed_size, max_position_embeddings, num_heads, num_layers, mlp_units, dropout_rate):\n        super(SemanticMatchingModel, self).__init__()\n\n        self.embedding_layer = TransformerEmbedding(vocab_size, embed_size, max_position_embeddings)\n        self.transformer_encoder = TransformerEncoder(vocab_size, embed_size, max_position_embeddings, num_heads, num_layers, mlp_units, dropout_rate)\n        self.pooling_layer = tf.keras.layers.GlobalAveragePooling1D()\n        self.output_layer = Dense(1, activation='sigmoid')\n\n    def call(self, inputs, training=None):\n        input_ids, segment_ids = inputs\n\n        # 前向传播\n        embeddings = self.embedding_layer([input_ids, segment_ids])\n        transformer_output = self.transformer_encoder([input_ids, segment_ids], training=training)\n        pooled = self.pooling_layer(transformer_output)\n        outputs = self.output_layer(pooled)\n\n        return outputs\n\n# 创建模型\nvocab_size = len(vocab)\nembed_size = 300\nmax_position_embeddings = 512\nnum_heads = 6\nnum_layers = 4\nmlp_units = 512\ndropout_rate = 0.1\n\nsemantic_matching_model = SemanticMatchingModel(\n    vocab_size, embed_size, max_position_embeddings, num_heads, num_layers, mlp_units, dropout_rate\n)\n","metadata":{},"execution_count":null,"outputs":[]}]}